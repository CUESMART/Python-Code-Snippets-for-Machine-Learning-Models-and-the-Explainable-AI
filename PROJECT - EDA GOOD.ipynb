{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ba00bb",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessay libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456d0e4",
   "metadata": {},
   "source": [
    "We have just imported python libraries that will be necessary for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea278371",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a69145",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to read the dataset data\n",
    "df = pd.read_csv('Student_Data.csv')\n",
    "\n",
    "# to view the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f33a9",
   "metadata": {},
   "source": [
    "Here, we loaded our dataset and view the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d95162",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "  \n",
    "# label_encoder object knows  \n",
    "# how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "df['Target']= label_encoder.fit_transform(df['Target']) \n",
    "  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf44bd3",
   "metadata": {},
   "source": [
    "We applied LabelEncoder which is a practical tool for preprocessing categorical data for machine learning models, ensuring the data is in a suitable numeric format for processing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95f38f",
   "metadata": {},
   "source": [
    "## Checking the details of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of numerical data types to look out for\n",
    "numerics_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "# list of categorical data types to look out for\n",
    "categorical_dtypes = ['object', 'category', 'bool']\n",
    "\n",
    "# get columns with numerical values\n",
    "has_numerical = [e for e in df.columns if df[e].dtype.name in numerics_dtypes]\n",
    "\n",
    "# get columns with categorical values\n",
    "has_categorical = [e for e in df.columns if df[e].dtype.name in categorical_dtypes]\n",
    "\n",
    "# print out the output\n",
    "print('Columns with numerical values:', ', '.join(has_numerical))\n",
    "print('\\nColumns with categorical values:', ', '.join(has_categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d81a3a",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42245830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the dimension of the dataset\n",
    "display(Markdown(f'This dataset contains {df.shape[0]} rows and {df.shape[1]} columns. Out of which, 1 is the target variable and the remaining are the independent variables.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb08acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the columns of the dataset\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632929ad",
   "metadata": {},
   "source": [
    "Here, we try to have a proper view of the columns in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad943961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the data type of the columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ', '.join(str(e) for e in df.dtypes.unique().tolist())\n",
    "display(Markdown(f'There are {len(df.dtypes.unique().tolist())} different types of data ({data_types}) present in the dataset.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f70abf",
   "metadata": {},
   "source": [
    "## Checking Null or Missing Values in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be59f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2d3bd",
   "metadata": {},
   "source": [
    "The above result indicates there is no missing value in our dataset. Let visualize this through the heatmap below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a927310",
   "metadata": {},
   "source": [
    "### Visualizing the null/missing values using heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec389cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize the null values using heatmap\n",
    "sns.heatmap(df.isnull(), cmap = \"cool_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08225e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_text = f'And we can clearly visualize that there are {sum(df.isnull().sum().tolist())} missing data present.' if sum(df.isnull().sum().tolist()) > 0 else 'And we can clearly visualize that there is no missing data present.'\n",
    "display(Markdown(null_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd18c6",
   "metadata": {},
   "source": [
    "The above function (df.info()), gives a brief information about the dataset which includes indexing type, column type, null values and memory usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b68874",
   "metadata": {},
   "source": [
    "## Lets see how many unique values present in each column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the total number of unique values and thier data type present in each column\n",
    "unique_values = []\n",
    "for column in df.columns.tolist():\n",
    "    unique_values.append([column, df[column].nunique(), df[column].dtype.name])\n",
    "    \n",
    "unique_df = pd.DataFrame(unique_values, columns=[['Column Name', 'Number of Unique Value', 'Column Data Type']])\n",
    "unique_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875aa75",
   "metadata": {},
   "source": [
    "The above table shows the number of unique values present in each column in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80fe4e",
   "metadata": {},
   "source": [
    "## Lets check the Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aadf556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the list of the target variable\n",
    "target_values = df[\"Target\"].unique().tolist()\n",
    "target_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78837d36",
   "metadata": {},
   "source": [
    "There are three (3) Categories in our Target variable column. These are Dropout = 0, Enrolled = 1, and Graduate = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da34d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"These are {len(target_values)} categories present in the target column namely {' and '.join([str(e) for e in target_values])}.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f790833",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef6871",
   "metadata": {},
   "source": [
    "this is or target variable count showing class imbalance as can be seen in their total count above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955af036",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declared an empty list\n",
    "skewed_right = []\n",
    "skewed_left = []\n",
    "found_outliers = []\n",
    "outliers_columns = []\n",
    "\n",
    "\n",
    "def find_outliers(column):\n",
    "    global found_outliers, outliers_columns\n",
    "    # finding the 1st quartile\n",
    "    q1 = df[column].quantile(0.25)\n",
    "\n",
    "    # finding the 3rd quartile\n",
    "    q3 = df[column].quantile(0.75)\n",
    "\n",
    "    # finding the iqr region\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # finding upper and lower whiskers\n",
    "    upper_bound = q3 + (1.5 * iqr)\n",
    "    lower_bound = q1 - (1.5 * iqr)\n",
    "\n",
    "    # Get the array data for column\n",
    "    arr1 = df[column]\n",
    "\n",
    "    # Get the outliers using the upper and lower whiskers\n",
    "    outliers = arr1[(arr1 <= lower_bound) | (arr1 >= upper_bound)]\n",
    "    \n",
    "    if len(outliers.values) > 0:\n",
    "        # append the found oultiers and the column name\n",
    "        found_outliers.append(outliers.tolist())\n",
    "        outliers_columns.append(column)\n",
    "        \n",
    "    return\n",
    "            \n",
    "def summary_table():\n",
    "    for column in df.columns:\n",
    "        # calculate the mean value\n",
    "        mean_value = df[column].mean(axis=0)\n",
    "    \n",
    "        # calculate the median value\n",
    "        median_value = df[column].median(axis=0)\n",
    "    \n",
    "        # check if colmun is skewed to the right\n",
    "        if mean_value > median_value:\n",
    "            skewed_right.append(column)\n",
    "    \n",
    "        # check if colmun is skewed to the left\n",
    "        elif mean_value < median_value:\n",
    "            skewed_left.append(column)\n",
    "        \n",
    "        # call the function to check for outliers\n",
    "        find_outliers(column)\n",
    "    \n",
    "    # store the summary description text\n",
    "    result_str = \"This gives the statistical information of the numerical columns. The summary of the dataset looks perfect since there is no negative/invalid values present.\" + \"\\n\" + \\\n",
    "    \"1. The counts of all the columns are the same which means there are no missing values in the dataset.\" + \"\\n\" + \\\n",
    "    f\"2. The mean value is greater than median (50%) in '{', '.join([str(e) for e in skewed_right])}' columns which means the data is skewed to right in these column.\" + \"\\n\" + \\\n",
    "    f\"3. The data in the '{', '.join([str(e) for e in skewed_left])}' columns have mean value less tha median value which means the data is skewed to the left.\"\n",
    "    \n",
    "    # check if there were any outlier found and add text to description text\n",
    "    if len(found_outliers[0]) > 0:\n",
    "        result_str += \"\\n\" + f\"4. By summarizing the data we can observe that there is a huge difference between 75% and max in '{', '.join([str(e) for e in outliers_columns])}' columns hence there are outliers present in the data.\"\n",
    "    \n",
    "    else:\n",
    "        result_str += \"\\n\" + '4. By summarizing the data we can observe that there is no huge difference between 75% and max hence there are no outliers present in the data.'\n",
    "    \n",
    "    # add the final text to description text\n",
    "    result_str += \"\\n\" + \"5. We can also notice the standard deviation, min, 25% percentile values from this described method.\"\n",
    "    \n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical summary of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the summary text using markdown\n",
    "display(Markdown(summary_table()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ca51e",
   "metadata": {},
   "source": [
    "lets visualize this here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ec70d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to check the distribution of the remaining columns \n",
    "plot_number = 1\n",
    "\n",
    "num_plots = len(has_numerical) - 1 if len(has_numerical)%2 != 0 else len(has_numerical)\n",
    "\n",
    "plt.figure(figsize = (num_plots + 4, num_plots * 2), facecolor = \"white\")\n",
    "\n",
    "outliers_columns = []\n",
    "\n",
    "for column in has_numerical:\n",
    "    # finding the 1st quartile\n",
    "    q1 = df[column].quantile(0.25)\n",
    "\n",
    "    # finding the 3rd quartile\n",
    "    q3 = df[column].quantile(0.75)\n",
    "\n",
    "    # finding the iqr region\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # finding upper and lower whiskers\n",
    "    upper_bound = q3 + (1.5 * iqr)\n",
    "    lower_bound = q1 - (1.5 * iqr)\n",
    "\n",
    "    # Get the array data for column\n",
    "    arr1 = df[column]\n",
    "\n",
    "    # Get the outliers using the upper and lower whiskers\n",
    "    outliers = arr1[(arr1 <= lower_bound) | (arr1 >= upper_bound)]\n",
    "    \n",
    "    if len(outliers.values) > 0:\n",
    "        # append the found oultiers and the column name\n",
    "        found_outliers.append(outliers.tolist())\n",
    "        outliers_columns.append(column)\n",
    "        \n",
    "    # plot the boxplots\n",
    "    if plot_number <= num_plots:\n",
    "        ax = plt.subplot(int(num_plots/2), 2, plot_number)\n",
    "        sns.boxplot(df[column], palette = \"Set2_r\")\n",
    "        plt.xlabel(column, fontsize = 14)\n",
    "        plt.yticks(rotation = 0, fontsize = 14)\n",
    "        \n",
    "    plot_number += 1\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a229ebc",
   "metadata": {},
   "source": [
    "There are outliers in the dataset as we can see from the above plot and we are mindful they might have undue influence on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the columns to remove outliers if necessary\n",
    "# columns_to_remove_outliers = ['column1', 'columns2','...']\n",
    "columns_to_remove_outliers = outliers_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ed650",
   "metadata": {},
   "source": [
    "## Hybrid outliers Handling Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to treat outliers\n",
    "def treat_outliers(df, columns):\n",
    "    try:\n",
    "        # loop through all column list\n",
    "        for column in columns:\n",
    "            # finding the 1st quartile\n",
    "            q1 = df[column].quantile(0.25)\n",
    "\n",
    "            # finding the 3rd quartile\n",
    "            q3 = df[column].quantile(0.75)\n",
    "\n",
    "            # get the column mean and median values\n",
    "            mean = df[column].mean()\n",
    "            # median = df[column].median()\n",
    "\n",
    "            # finding the iqr region\n",
    "            iqr = q3 - q1\n",
    "\n",
    "            # finding upper and lower whiskers\n",
    "            upper_bound = q3 + (1.5 * iqr)\n",
    "            lower_bound = q1 - (1.5 * iqr)\n",
    "\n",
    "            # Get the array data for column\n",
    "            arr1 = df[column]\n",
    "\n",
    "            # Get the outliers using the upper and lower whiskers\n",
    "            outliers = arr1[(arr1 <= lower_bound) | (arr1 >= upper_bound)]\n",
    "\n",
    "            # get the min value of the outliers found\n",
    "            min_value = round(min(outliers.values)) if len(outliers.values) > 0 else 0\n",
    "\n",
    "            # calculate the percentage of the outliers found\n",
    "            percentage_value = round((len(outliers.values) / len(arr1)) * 100, 2) if len(outliers.values) > 0 else 0\n",
    "\n",
    "            # do this if outliers is less than or equal to 5%\n",
    "            if round(percentage_value) <= 5 and len(outliers.values) > 0:\n",
    "                # capping Outliers using IQR Ranges\n",
    "                df.loc[(df[column] <= lower_bound), column] = lower_bound\n",
    "                df.loc[(df[column] >= upper_bound), column] = upper_bound\n",
    "\n",
    "            # do this if outliers is greater than 5%\n",
    "            elif round(percentage_value) > 5 and len(outliers.values) > 0:\n",
    "                # replacing outlier values with the mean or median value\n",
    "                df.loc[(df[column] <= lower_bound), column] = mean\n",
    "                df.loc[(df[column] >= upper_bound), column] = mean\n",
    "\n",
    "\n",
    "        # return \n",
    "        return \n",
    "\n",
    "    except BaseException as error:\n",
    "        print('\\nPlease ensure the dataframe name is correct and the target column is entered correctly: {}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a150cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treating outliers\n",
    "treat_outliers(df, columns_to_remove_outliers)\n",
    "\n",
    "# to check the distribution of the remaining columns \n",
    "plot_number = 1\n",
    "\n",
    "num_plots = len(columns_to_remove_outliers) - 1 if len(columns_to_remove_outliers)%2 != 0 else len(columns_to_remove_outliers)\n",
    "\n",
    "plt.figure(figsize = (num_plots + 4, num_plots * 2), facecolor = \"white\")\n",
    "\n",
    "for column in columns_to_remove_outliers:     \n",
    "    # plot the boxplots\n",
    "    if plot_number <= num_plots:\n",
    "        ax = plt.subplot(int(num_plots/2), 2, plot_number)\n",
    "        sns.boxplot(df[column], palette = \"Set2_r\")\n",
    "        plt.xlabel(column, fontsize = 14)\n",
    "        plt.yticks(rotation = 0, fontsize = 14)\n",
    "        \n",
    "    plot_number += 1\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8dd69a",
   "metadata": {},
   "source": [
    "As we can see from the above plots, the columns with oultiers have been treated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10196f",
   "metadata": {},
   "source": [
    "### Encoding categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical columns using OrdinalEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "OE = OrdinalEncoder()\n",
    "for i in df.columns:\n",
    "    if df[i].dtypes == 'object':\n",
    "        df[i] = OE.fit_transform(df[i].values.reshape(-1, 1))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59561bbb",
   "metadata": {},
   "source": [
    "We have converted the categorical columns into numerical columns using the Ordinal Encoding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical summary of numerial\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeebd5d",
   "metadata": {},
   "source": [
    "After encoding the categorical columns we can see all column details here. The counts of all the columns are the same which means there are no null values in the data. This describe method offers insight into more details of the count, mean, std, min, IQR and max values of all the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b29809",
   "metadata": {},
   "source": [
    "## Visualizing the correlation between label and features using bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (22, 7))\n",
    "df.corr()[\"Target\"].sort_values(ascending = False).drop([\"Target\"]).plot(kind='bar', color=\"m\")\n",
    "plt.xlabel('Feature', fontsize = 15)\n",
    "plt.ylabel('Target', fontsize = 15)\n",
    "plt.title('Correlation between label and features using barplot', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad9a25",
   "metadata": {},
   "source": [
    "The above barplot displays the correlation between the label and the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c9b9c",
   "metadata": {},
   "source": [
    "### Seperating features and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0262b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(\"Target\", axis = 1)\n",
    "y = df[\"Target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25198b70",
   "metadata": {},
   "source": [
    "Defining and seperating the features and Target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c35ad8",
   "metadata": {},
   "source": [
    "### Feature Scaling using Standard Scalarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x = pd.DataFrame(scaler.fit_transform(x), columns = x.columns)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d384796",
   "metadata": {},
   "source": [
    "We have scaled the data using Standard Scalarization method to overcome the issue of biaseness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4516c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising the target variable unique value\n",
    "ax = sns.countplot(x=y, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70724d4d",
   "metadata": {},
   "source": [
    "## Model building (without SMOTE) and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770376dd",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size = 0.30, random_state = 10)\n",
    "\n",
    "# create the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59afad38",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9681bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 10)\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01d4ae",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456bdd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f2f06",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc798e8",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = SVC()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1636e49d",
   "metadata": {},
   "source": [
    "## K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15887db",
   "metadata": {},
   "source": [
    "## Model comparison (without SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.datasets import load_iris  # Example dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Import the classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize models\n",
    "models = [\n",
    "    (\"Logistic Regression\", make_pipeline(StandardScaler(), LogisticRegression())),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier()),\n",
    "    (\"SVM\", make_pipeline(StandardScaler(), SVC(probability=True))),\n",
    "    (\"KNN\", make_pipeline(StandardScaler(), KNeighborsClassifier())),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "# Prepare a list for storing results\n",
    "results = []\n",
    "\n",
    "# Train, predict, and evaluate each model\n",
    "for name, model in models:\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Collecting metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Appending results\n",
    "    results.append([name, accuracy, f1, precision, recall])\n",
    "\n",
    "# Creating a DataFrame from the results\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall'])\n",
    "\n",
    "# Displaying the DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e9ec9",
   "metadata": {},
   "source": [
    "## Oversampling Method (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling the data\n",
    "SM = SMOTE()\n",
    "x1, y1 = SM.fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0031e",
   "metadata": {},
   "source": [
    "We applied SMOTE here. SMOTE which stands for Synthetic Minority Over-sampling Technique, is a powerful technique for dealing with unbalanced dataset by generating synthetic samples, thus facilitating more robust and accurate machine learning models, especially in the cases where the minority class is of great interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising the target variable unique value\n",
    "ax = sns.countplot(x=y1, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9991a0",
   "metadata": {},
   "source": [
    "We now have a balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b20d0",
   "metadata": {},
   "source": [
    "## Model Building with SMOTE and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba14b9",
   "metadata": {},
   "source": [
    "We aim to build six machine learning models, see the performance of each model and select the best performed model for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb0e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e8f3c",
   "metadata": {},
   "source": [
    "## Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size = 0.30, random_state = 10)\n",
    "\n",
    "# create the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e8676",
   "metadata": {},
   "source": [
    "We divided our dataset ino training (70%) and testing(30%), builded and evaluated Logistic Regression Model with above displayed metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee02c424",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad170292",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efedf34a",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dcb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3420f9",
   "metadata": {},
   "source": [
    "## Survey Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000419e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = SVC()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a25c5f",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27846252",
   "metadata": {},
   "source": [
    "We have trained and evaluated six machine learning models which are Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, Survey Vector Machine and K-Nearest neighbors. The performance of each model can be seen in their accuracies, precisions, recalls and F1-scores as displayed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c42b48",
   "metadata": {},
   "source": [
    "## Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.datasets import load_iris  # Example dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Import the classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize models\n",
    "models = [\n",
    "    (\"Logistic Regression\", make_pipeline(StandardScaler(), LogisticRegression())),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier()),\n",
    "    (\"SVM\", make_pipeline(StandardScaler(), SVC(probability=True))),\n",
    "    (\"KNN\", make_pipeline(StandardScaler(), KNeighborsClassifier())),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "# Prepare a list for storing results\n",
    "results = []\n",
    "\n",
    "# Train, predict, and evaluate each model\n",
    "for name, model in models:\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Collecting metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Appending results\n",
    "    results.append([name, accuracy, f1, precision, recall])\n",
    "\n",
    "# Creating a DataFrame from the results\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall'])\n",
    "\n",
    "# Displaying the DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f07c8",
   "metadata": {},
   "source": [
    "By comparing the performance of the six models, we can see that Random Forest ranked the best performing model with an accuracy of 83%. we can see that all the models generalized well as they maintain a balanced data across other metrics like F1-scores, precisions Recall etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd775a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1.iloc[:, [3, 6, 12, 13, 22, 24, 25, 28, 30, 31]]\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae673fc",
   "metadata": {},
   "source": [
    "Here, we selected features that have strong influence or correlation with the target variable. The next is to retrain our models with the selected 10 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6adf8",
   "metadata": {},
   "source": [
    "# Retraining the Models on the Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1870fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376de4e",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# create the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b57433f",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc5a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size = 0.20, random_state = 10)\n",
    "\n",
    "# create the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07700c4c",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size = 0.20, random_state = 10)\n",
    "\n",
    "# create the model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d350e",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98115698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size = 0.20, random_state = 10)\n",
    "\n",
    "# create the model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bc5a1",
   "metadata": {},
   "source": [
    "## Survey Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# create the model\n",
    "model = SVC()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f602bb",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9251a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# create the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "prediction = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed8f39",
   "metadata": {},
   "source": [
    "We can see that after retraining the models with selected features, Random forest still merged the best performed model. The next si to tune our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6c85e",
   "metadata": {},
   "source": [
    "## Models Comparison with the Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.datasets import load_iris  # Example dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Import the classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Initialize models\n",
    "models = [\n",
    "    (\"Logistic Regression\", make_pipeline(StandardScaler(), LogisticRegression())),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier()),\n",
    "    (\"SVM\", make_pipeline(StandardScaler(), SVC(probability=True))),\n",
    "    (\"KNN\", make_pipeline(StandardScaler(), KNeighborsClassifier())),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "# Prepare a list for storing results\n",
    "results = []\n",
    "\n",
    "# Train, predict, and evaluate each model\n",
    "for name, model in models:\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Collecting metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Appending results\n",
    "    results.append([name, accuracy, f1, precision, recall])\n",
    "\n",
    "# Creating a DataFrame from the results\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall'])\n",
    "\n",
    "# Displaying the DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a148d4",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2228734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Model definition\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Setup KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "for train_index, test_index in kf.split(x1):\n",
    "    x_train, x_test = x1.iloc[train_index], x1.iloc[test_index]\n",
    "    y_train, y_test = y1[train_index], y1[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions and evaluate\n",
    "    predictions = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    scores.append(accuracy)\n",
    "\n",
    "# Convert scores to a numpy array for mean and std calculations\n",
    "scores = np.array(scores)\n",
    "\n",
    "# Print the accuracy for each fold\n",
    "print(f\"Accuracy for each fold: {scores}\")\n",
    "\n",
    "# Print the mean accuracy and standard deviation\n",
    "print(f\"Mean accuracy: {scores.mean()}, Standard deviation: {scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc8469",
   "metadata": {},
   "source": [
    "### AUC-ROC Scores and Multi-class ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define and fit the model \n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict class labels for the test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Predict probabilities for all classes\n",
    "y_probs = model.predict_proba(x_test)\n",
    "\n",
    "# Calculate AUC-ROC Score\n",
    "# For multiclass, calculate ROC AUC for each class and average them\n",
    "roc_auc = roc_auc_score(y_test, y_probs, multi_class=\"ovr\")\n",
    "\n",
    "# Calculate metrics using 'macro' average for multiclass data\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"AUC-ROC: {roc_auc}\")\n",
    "\n",
    "# Plotting ROC Curve for Multiclass  one-vs-rest\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "n_class = len(set(y_test))\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_probs[:,i], pos_label=i)\n",
    "    \n",
    "plt.figure(figsize=(6, 4))\n",
    "for i in range(n_class):\n",
    "    plt.plot(fpr[i], tpr[i], linestyle='--', label=f'Class {i} vs Rest')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03554e",
   "metadata": {},
   "source": [
    "## ROC for Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Binarize the output labels for multi-class classification\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2]) \n",
    "# Compute ROC curve and ROC area for each class\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_scores = model.predict_proba(x_test)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_scores[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "for i, color in enumerate(colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for Multi-Class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e2289",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# As training data in x_train and labels in y_train\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Get model predictions\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6548ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# As training data in x_train and labels in y_train\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Use the 'model' variable to access feature_importances_ since that's where your trained model is stored\n",
    "feature_importances = model.feature_importances_\n",
    "feature_names = x_train.columns\n",
    "\n",
    "# Create a pandas series to visualize importance\n",
    "importances = pd.Series(feature_importances, index=feature_names)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "importances.nlargest(10).sort_values().plot(kind='barh')  # adjust the number as needed\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('F Score')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1990d",
   "metadata": {},
   "source": [
    "we tuned the best performed model with GridsearchCV to avoid model biaseness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0ec96",
   "metadata": {},
   "source": [
    "# Explainable (XAI) Machine Learning Model\n",
    "Explainable Artificial Intelligence (XAI) refers to methods and techniques in the field of artificial intelligence (AI) that make the results and operations of AI systems understandable and interpretable to humans. Therefore, we deploy SHAP to achieve this.\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a game theory-based approach for explaining the output of any machine learning model. It offers insights into how each feature in your dataset contributes to the prediction for each individual observation. SHAP values provide a measure of the impact of each feature on the prediction, compared to the average prediction for the dataset. This approach is grounded in the principles of cooperative game theory and offers a consistent and fair method to distribute the \"payout\" (i.e., the prediction) among the \"players\" (i.e., the features)\n",
    "## Why SHAP?\n",
    "### Individual Prediction Explanation:\n",
    "SHAP can explain the output of the model for individual predictions, which is especially useful in applications where understanding why a model made a specific prediction is important.\n",
    "### Global Understanding:\n",
    "SHAP also provides global insights by aggregating individual explanations, helping to understand how features generally impact predictions across the dataset.\n",
    "### Consistency and Fairness:\n",
    "SHAP values have desirable properties like consistency and local accuracy, which support fairness and transparency in model explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca62ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# As training data in x_train and labels in y_train\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# The trained RandomForestClassifier\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(x_test)  # x_test is your test set features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f701fc5",
   "metadata": {},
   "source": [
    "## Shap Summary Plot for All Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2127341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot for all classes\n",
    "shap.summary_plot(shap_values, x_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5b6fc",
   "metadata": {},
   "source": [
    "the above summary plot unveils the average impact of each feature in a particular class on the model prediction. This offers more insight on which features contributing to either positively or negatively to why the model made decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8f940",
   "metadata": {},
   "source": [
    "## Shap Summary Plot for Class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For class 2\n",
    "shap.summary_plot(shap_values[2], x_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f0b6f",
   "metadata": {},
   "source": [
    "## Shap Summary Plot for Class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baab8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For class 1\n",
    "shap.summary_plot(shap_values[1], x_test, plot_type=\"bar\", color='green')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59933654",
   "metadata": {},
   "source": [
    "## Shap Summary Plot for Class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ab9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For class 0\n",
    "shap.summary_plot(shap_values[0], x_test, plot_type=\"bar\", color='pink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301c2a2",
   "metadata": {},
   "source": [
    "From the three (3) barplot above, we can see Shap summary plot on each of the three classes showing clearly, the contribution of each feature in across the 3 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0620a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize JavaScript visualization\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b8017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TreeExplainer object\n",
    "explainer = shap.TreeExplainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values for x_test\n",
    "shap_values = explainer.shap_values(x_test)\n",
    "\n",
    "# Define class names for readability\n",
    "class_names = ['DROPOUT', 'ENROLLED', 'GRADUATE']\n",
    "\n",
    "# Generate and display force plots for each class for the first instance in x_test\n",
    "for class_idx in range(model.n_classes_):\n",
    "    # Display the force plot for the first instance in the test set\n",
    "    shap.force_plot(\n",
    "        base_value=explainer.expected_value[class_idx], \n",
    "        shap_values=shap_values[class_idx][0],         \n",
    "        features=x_test.iloc[0,:],                      \n",
    "        feature_names=x1.columns.tolist(),              \n",
    "        matplotlib=True                              \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a432fa",
   "metadata": {},
   "source": [
    "The Shap force plots for the 3 classes above indicates the impact of each feature on the predictions. those on red indicates they are pushing higher while blue shows lower impacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714901d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adb29c15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1bd8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Curricular units 2nd sem (approved)', shap_values[2], x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36576200",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Curricular units 1st sem (approved)', shap_values[2], x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa67203",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c55a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "674fa3f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128982fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
